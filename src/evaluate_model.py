# evaluate_model.py
# ê°œë³„ ëª¨ë¸ì˜ KMMLU ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# WandB ë¡œê¹… ë° 45ê°œ ì „ì²´ ê³¼ëª© ìˆœìœ„ ì €ì¥ ê¸°ëŠ¥ í¬í•¨

# === í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ===
from lm_eval import simple_evaluate
import csv
import json
import os
from datetime import datetime
import torch
import gc
import re
import time

# WandB ì„í¬íŠ¸ (ì„ íƒì )
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸ wandb not installed. To enable wandb logging, run: pip install wandb")

# === ì „ì—­ ìƒìˆ˜ ì •ì˜ ===
CSV_FILE = 'kmmlu_results.csv'
JSON_LEADERBOARD = 'kmmlu_leaderboard.json'

# === KMMLU 45ê°œ ê³¼ëª©ì˜ ì •í™•í•œ ëŒ€ë¶„ë¥˜ ë§¤í•‘ ===
KMMLU_SUBJECT_MAPPING = {
    "STEM": [
        "math", "physics", "chemistry", "biology", "earth_science",
        "computer_science", "electrical_engineering", "mechanical_engineering",
        "chemical_engineering", "civil_engineering", "information_technology"
    ],
    "HUMSS": [
        "korean_history", "world_history", "geography", 
        "korean_language_and_literature", "philosophy",
        "political_science_and_sociology", "economics", "law",
        "educational_psychology", "social_welfare",
        "human_development_and_family_studies", "business_administration",
        "accounting", "marketing"
    ],
    "Applied Science": [
        "agricultural_sciences", "food_processing", "animal_sciences",
        "fashion", "aviation_engineering_and_maintenance", "health",
        "nursing", "medicine", "dentistry", "pharmacology",
        "korean_medicine", "construction"
    ],
    "Other": [
        "public_safety", "defense", "nondestructive_testing",
        "industrial_engineer", "taxation", "labor_law",
        "patent", "real_estate"
    ]
}

# ì—­ ë§¤í•‘: ê³¼ëª©ëª… â†’ ëŒ€ë¶„ë¥˜
SUBJECT_TO_CATEGORY = {}
for category, subjects in KMMLU_SUBJECT_MAPPING.items():
    for subject in subjects:
        SUBJECT_TO_CATEGORY[subject] = category

def parse_model_config(model_args):
    """
    model_args ë¬¸ìì—´ì—ì„œ ë¹„íŠ¸ ì •ë°€ë„ ì¶”ì¶œ
    
    Args:
        model_args: "pretrained=...,load_in_8bit=True,dtype=float16" í˜•ì‹ì˜ ë¬¸ìì—´
    
    Returns:
        str: ë¹„íŠ¸ ì •ë°€ë„ (ì˜ˆ: '8bit', 'float16', '4bit')
    """
    # load_in_8bit=True ì²´í¬
    if 'load_in_8bit=True' in model_args:
        return '8bit'
    # load_in_4bit=True ì²´í¬
    elif 'load_in_4bit=True' in model_args:
        return '4bit'
    # dtype íŒŒë¼ë¯¸í„° ì²´í¬
    elif 'dtype=' in model_args:
        dtype_match = re.search(r'dtype=(\w+)', model_args)
        if dtype_match:
            return dtype_match.group(1)
    
    return 'unknown'

def format_time(seconds):
    """
    ì´ˆ ë‹¨ìœ„ ì‹œê°„ì„ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    Args:
        seconds: ì´ˆ ë‹¨ìœ„ ì‹œê°„
    
    Returns:
        str: í˜•ì‹í™”ëœ ì‹œê°„ ë¬¸ìì—´ (ì˜ˆ: "1h 23m 45s", "45m 30s", "30s")
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    
    if hours > 0:
        return f"{hours}h {minutes}m {secs}s"
    elif minutes > 0:
        return f"{minutes}m {secs}s"
    else:
        return f"{secs}s"

def get_subject_category(subject_name, results=None):
    """
    ê°œë³„ ê³¼ëª©ì´ ì–´ë–¤ ëŒ€ë¶„ë¥˜ì— ì†í•˜ëŠ”ì§€ ì •í™•í•˜ê²Œ íŒë‹¨
    """
    normalized = subject_name.replace('kmmlu_', '').lower()
    category = SUBJECT_TO_CATEGORY.get(normalized)
    
    if category:
        return category
    
    for cat, subjects in KMMLU_SUBJECT_MAPPING.items():
        for subj in subjects:
            if subj.replace('_', ' ') in normalized.replace('_', ' '):
                return cat
            if normalized.replace('_', ' ') in subj.replace('_', ' '):
                return cat
    
    print(f"âš ï¸ Warning: Unknown subject '{subject_name}', defaulting to 'Other'")
    return 'Other'

def load_results():
    """ì €ì¥ëœ í‰ê°€ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸° (CSVì—ì„œ)"""
    results = []
    if os.path.exists(CSV_FILE):
        with open(CSV_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if not row.get('Model') or row.get('Model').startswith('---'):
                    continue
                results.append(row)
    return results

def save_results(results):
    """í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    save_to_csv(results)

def save_to_csv(results):
    """
    í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    êµ¬ì¡°: [ê¸°ë³¸ ì •ë³´ ì„¹ì…˜] + [ì „ì²´ ê³¼ëª© ìˆœìœ„ ì„¹ì…˜]
    """
    if not results:
        return
    
    with open(CSV_FILE, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # === ì„¹ì…˜ 1: ê¸°ë³¸ ëª¨ë¸ ì •ë³´ ===
        writer.writerow(['=== BASIC MODEL INFORMATION ==='])
        writer.writerow([])
        
        # í—¤ë”: Timestamp ì œê±°, Elapsed_Time, Batch_Size, Precision ì¶”ê°€
        basic_header = ['Model', 'Overall', 'STEM', 'HUMSS', 'Applied', 'Other', 
                       'Best_Subject', 'Best_Score', 
                       'Worst_Subject', 'Worst_Score',
                       'Elapsed_Time', 'Batch_Size', 'Precision']
        writer.writerow(basic_header)
        
        # ê° ëª¨ë¸ì˜ ê¸°ë³¸ ì •ë³´
        for result in results:
            basic_row = [
                result.get('model', result.get('Model', '')),
                f"{result['overall']:.4f}",
                f"{result['stem']:.4f}",
                f"{result['humss']:.4f}",
                f"{result['applied']:.4f}",
                f"{result['other']:.4f}",
                result['best']['name'],
                f"{result['best']['score']:.4f}",
                result['worst']['name'],
                f"{result['worst']['score']:.4f}",
                result.get('elapsed_time', 'N/A'),  # ê±¸ë¦° ì‹œê°„
                result.get('batch_size', 'N/A'),    # ë°°ì¹˜ í¬ê¸°
                result.get('precision', 'N/A')      # ë¹„íŠ¸ ì •ë°€ë„
            ]
            writer.writerow(basic_row)
        
        # === ì„¹ì…˜ 2: ì „ì²´ ê³¼ëª© ìˆœìœ„ (45ê°œ) ===
        writer.writerow([])
        writer.writerow(['=== ALL SUBJECTS RANKING ==='])
        writer.writerow([])
        
        for result in results:
            model_name = result.get('model', result.get('Model', ''))
            
            writer.writerow([f'Model: {model_name}'])
            writer.writerow(['Rank', 'Subject', 'Score', 'Category'])
            
            all_subjects = result.get('all_subjects_ranked', [])
            
            for rank, subject_info in enumerate(all_subjects, 1):
                writer.writerow([
                    rank,
                    subject_info['name'],
                    f"{subject_info['score']:.4f}",
                    subject_info['category']
                ])
            
            writer.writerow([])

def append_to_leaderboard(result_data):
    """
    í‰ê°€ ê²°ê³¼ë¥¼ JSON ë¦¬ë”ë³´ë“œì— ëˆ„ì  ì €ì¥
    
    Args:
        result_data: í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # ê¸°ì¡´ ë¦¬ë”ë³´ë“œ ë¶ˆëŸ¬ì˜¤ê¸°
    leaderboard = []
    if os.path.exists(JSON_LEADERBOARD):
        try:
            with open(JSON_LEADERBOARD, 'r', encoding='utf-8') as f:
                leaderboard = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            leaderboard = []
    
    # ê°™ì€ ëª¨ë¸ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
    existing_idx = None
    for i, entry in enumerate(leaderboard):
        if entry.get('model') == result_data['model'] or entry.get('model_path') == result_data['model_path']:
            existing_idx = i
            break
    
    # ê¸°ì¡´ ëª¨ë¸ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì¶”ê°€
    if existing_idx is not None:
        leaderboard[existing_idx] = result_data
        print(f"ğŸ“ Updated existing entry in leaderboard: {result_data['model']}")
    else:
        leaderboard.append(result_data)
        print(f"â• Added new entry to leaderboard: {result_data['model']}")
    
    # JSON íŒŒì¼ì— ì €ì¥
    with open(JSON_LEADERBOARD, 'w', encoding='utf-8') as f:
        json.dump(leaderboard, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ Leaderboard saved to {JSON_LEADERBOARD}")

def evaluate_model(model_name, model_args, label, batch_size=16, wandb_project=None, wandb_run_name=None):
    """
    ëª¨ë¸ í‰ê°€ ë° ì €ì¥
    """
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    gc.collect()
    
    # ë¹„íŠ¸ ì •ë°€ë„ ì¶”ì¶œ
    precision = parse_model_config(model_args)
    
    print(f"\n{'='*60}")
    print(f"Evaluating: {label}")
    print(f"Batch Size: {batch_size}")
    print(f"Precision: {precision}")
    print(f"{'='*60}")
    
    # WandB ì´ˆê¸°í™”
    wandb_run = None
    if WANDB_AVAILABLE and wandb_project:
        try:
            wandb_run = wandb.init(
                project=wandb_project,
                name=wandb_run_name or label,
                config={
                    "model_name": model_name,
                    "model_args": model_args,
                    "label": label,
                    "batch_size": batch_size,
                    "precision": precision
                },
                reinit=True
            )
            print(f"âœ… WandB logging enabled: {wandb_project}/{wandb_run_name or label}")
        except Exception as e:
            print(f"âš ï¸ WandB initialization failed: {e}")
            wandb_run = None
    
    # CUDA ë©”ëª¨ë¦¬ ìµœì í™”
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    
    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    
    # í‰ê°€ ì‹¤í–‰
    results = simple_evaluate(
        model="hf",
        model_args=model_args,
        tasks=["kmmlu"],
        batch_size=batch_size,
        device="cuda:0",
        num_fewshot=5
    )
    
    # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ë° ê²½ê³¼ ì‹œê°„ ê³„ì‚°
    end_time = time.time()
    elapsed_seconds = end_time - start_time
    elapsed_time_str = format_time(elapsed_seconds)
    
    # ì „ì²´ í‰ê·  ì •í™•ë„
    overall = results['results']['kmmlu']['acc,none']
    
    # ëŒ€ë¶„ë¥˜ ì ìˆ˜
    stem_score = results['results']['kmmlu_stem']['acc,none']
    humss_score = results['results']['kmmlu_humss']['acc,none']
    applied_score = results['results']['kmmlu_applied_science']['acc,none']
    other_score = results['results']['kmmlu_other']['acc,none']
    
    # ê°œë³„ ê³¼ëª© ì ìˆ˜ ìˆ˜ì§‘
    all_subjects = {}
    for task, metrics in results['results'].items():
        if task.startswith('kmmlu_') and 'acc,none' in metrics:
            if task not in ['kmmlu_stem', 'kmmlu_humss', 'kmmlu_applied_science', 'kmmlu_other', 'kmmlu']:
                subject_name = task.replace('kmmlu_', '').replace('_', ' ').title()
                category = get_subject_category(task)
                all_subjects[subject_name] = {
                    'score': metrics['acc,none'],
                    'category': category
                }
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_subjects = sorted(all_subjects.items(), key=lambda x: x[1]['score'])
    
    all_subjects_ranked = [
        {
            "name": name,
            "score": info['score'],
            "category": info['category']
        }
        for name, info in sorted_subjects
    ]
    
    best_subject = sorted_subjects[-1]
    worst_subject = sorted_subjects[0]
    
    # ê²°ê³¼ ì •ë¦¬
    result_data = {
        "model": label,
        "model_path": model_name,
        "overall": overall,
        "stem": stem_score,
        "humss": humss_score,
        "applied": applied_score,
        "other": other_score,
        "best": {
            "name": best_subject[0],
            "score": best_subject[1]['score']
        },
        "worst": {
            "name": worst_subject[0],
            "score": worst_subject[1]['score']
        },
        "all_subjects_ranked": all_subjects_ranked,
        "elapsed_time": elapsed_time_str,  # ê±¸ë¦° ì‹œê°„ ì¶”ê°€
        "batch_size": batch_size,          # ë°°ì¹˜ í¬ê¸° ì¶”ê°€
        "precision": precision              # ë¹„íŠ¸ ì •ë°€ë„ ì¶”ê°€
    }
    
    # WandB ë¡œê¹…
    if wandb_run:
        try:
            wandb.log({
                "overall_accuracy": overall,
                "stem_accuracy": stem_score,
                "humss_accuracy": humss_score,
                "applied_science_accuracy": applied_score,
                "other_accuracy": other_score,
                "best_subject_score": best_subject[1]['score'],
                "worst_subject_score": worst_subject[1]['score'],
                "elapsed_seconds": elapsed_seconds,
                "batch_size": batch_size,
                "precision": precision
            })
            
            top_5 = all_subjects_ranked[-5:][::-1]
            bottom_5 = all_subjects_ranked[:5]
            
            wandb.log({
                "top_5_subjects": wandb.Table(
                    columns=["Subject", "Score", "Category"],
                    data=[[s['name'], s['score'], s['category']] for s in top_5]
                ),
                "bottom_5_subjects": wandb.Table(
                    columns=["Subject", "Score", "Category"],
                    data=[[s['name'], s['score'], s['category']] for s in bottom_5]
                )
            })
            
            print("âœ… Results logged to WandB")
        except Exception as e:
            print(f"âš ï¸ WandB logging failed: {e}")
        finally:
            wandb.finish()
    
    # ì½˜ì†” ì¶œë ¥
    print(f"\nâœ… Evaluation Complete!")
    print(f"Elapsed Time: {elapsed_time_str}")
    print(f"Overall: {overall:.2%}")
    
    print(f"\nğŸ“Š Category Scores:")
    print(f"  STEM:            {stem_score:.2%}")
    print(f"  HUMSS:           {humss_score:.2%}")
    print(f"  Applied Science: {applied_score:.2%}")
    print(f"  Other:           {other_score:.2%}")
    
    print(f"\nğŸ† Best Subject:  {best_subject[0]:35s} {best_subject[1]['score']:.2%} ({best_subject[1]['category']})")
    print(f"ğŸ“‰ Worst Subject: {worst_subject[0]:35s} {worst_subject[1]['score']:.2%} ({worst_subject[1]['category']})")
    
    print(f"\nğŸ“‹ Bottom 10 Subjects (ë‚®ì€ ì ìˆ˜):")
    for i, subject in enumerate(all_subjects_ranked[:10], 1):
        print(f"  {i:2d}. {subject['name']:35s} {subject['score']:.2%}  [{subject['category']}]")
    
    print(f"\nTotal Subjects Evaluated: {len(all_subjects)} (KMMLU has 45 standard subjects)")
    
    # ì €ì¥
    all_results = load_results()
    
    existing_idx = None
    for i, r in enumerate(all_results):
        if r.get('Model') == label or r.get('model_path') == model_name:
            existing_idx = i
            break
    
    if existing_idx is not None:
        all_results[existing_idx] = result_data
    else:
        all_results.append(result_data)
    
    # 1. CSV ì €ì¥
    save_results(all_results)
    print(f"\nğŸ’¾ Saved to {CSV_FILE}\n")
    # 2. JSONì— ëˆ„ì  ì €ì¥
    append_to_leaderboard(result_data)
    
    return result_data

if __name__ == "__main__":
    evaluate_model(
        model_name="naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B",
        model_args="pretrained=naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B,dtype=float16", # dtype=float16, load_in_8bit=True
        label="naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B",
        batch_size=64
    )