pretrained=pretrained=allganize/Llama-3-Alpha-Ko-8B-Instruct,load_in_8bit=True appears to be an instruct or chat variant but chat template
        is not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).
config.json: 100%|███████████████████████████████████████████████████████████████████| 772/772 [00:00<00:00, 1.15MB/s]
tokenizer_config.json: 51.0kB [00:00, 48.0MB/s]
tokenizer.json: 9.09MB [00:00, 24.8MB/s]
special_tokens_map.json: 100%|████████████████████████████████████████████████████████| 350/350 [00:00<00:00, 532kB/s]
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
model.safetensors.index.json: 23.9kB [00:00, 30.5MB/s]
model-00004-of-00004.safetensors: 100%|██████████████████████████████████████████| 1.17G/1.17G [00:22<00:00, 50.9MB/s]
model-00003-of-00004.safetensors: 100%|██████████████████████████████████████████| 4.92G/4.92G [01:02<00:00, 78.8MB/s]
model-00002-of-00004.safetensors: 100%|██████████████████████████████████████████| 5.00G/5.00G [01:03<00:00, 79.1MB/s]
model-00001-of-00004.safetensors: 100%|██████████████████████████████████████████| 4.98G/4.98G [01:36<00:00, 51.8MB/s]
Fetching 4 files: 100%|█████████████████████████████████████████████████████████████████| 4/4 [01:36<00:00, 24.10s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████| 4/4 [00:50<00:00, 12.62s/it]
generation_config.json: 100%|████████████████████████████████████████████████████████| 121/121 [00:00<00:00, 70.6kB/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 929.81it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 918.95it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 915.02it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 896.52it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 907.22it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 600/600 [00:00<00:00, 914.26it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 905.01it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 916.93it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 919.44it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 912.29it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 923.01it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 919.07it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 895.03it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 913.99it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 901.74it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 923.70it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 894.10it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 887.06it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 914.37it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 694.69it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 922.55it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 933.68it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 925.64it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 885.16it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 871.08it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 851.43it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 890.65it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 920.54it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 917.16it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 932.17it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 936.38it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 600/600 [00:00<00:00, 926.02it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 908.18it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 902.96it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 940.06it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 900.05it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 925.11it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 914.00it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 130/130 [00:00<00:00, 934.16it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 925.96it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 920.13it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 813.42it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 902.40it/s]
100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 878.72it/s]
100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 905.65it/s]
Running loglikelihood requests:   0%|                                                      | 0/140120 [00:00<?, ?it/s]/home/sllmdj05/.pyenv/versions/third/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Running loglikelihood requests: 100%|█████████████████████████████████████████| 140120/140120 [37:46<00:00, 61.81it/s]
