# evaluate_model.py
# ê°œë³„ ëª¨ë¸ì˜ KMMLU ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

# === í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ===
from lm_eval import simple_evaluate
import csv  # CSV íŒŒì¼ ì €ì¥ìš©
import os
from datetime import datetime
import torch
import gc

# === ì „ì—­ ìƒìˆ˜ ì •ì˜ ===
CSV_FILE = 'kmmlu_results.csv'  # CSV íŒŒì¼

def load_results():
    """ì €ì¥ëœ í‰ê°€ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸° (CSVì—ì„œ)"""
    results = []
    if os.path.exists(CSV_FILE):
        with open(CSV_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                results.append(row)
    return results

def save_results(results):
    """í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    save_to_csv(results)

def save_to_csv(results):
    """í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    if not results:
        return
    
    with open(CSV_FILE, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # í—¤ë” ì‘ì„±
        header = ['Model', 'Overall', 'STEM', 'HUMSS', 'Applied', 'Other', 
                 'Best_Subject', 'Best_Score']
        # ìµœí•˜ìœ„ 10ê°œ ê³¼ëª© í—¤ë” ì¶”ê°€
        for i in range(1, 11):
            header.extend([f'Worst_{i}_Subject', f'Worst_{i}_Score'])
        header.append('Timestamp')
        writer.writerow(header)
        
        # ë°ì´í„° ì‘ì„±
        for result in results:
            row = [
                result.get('model', result.get('Model', '')),
                f"{result['overall']:.4f}",
                f"{result['stem']:.4f}",
                f"{result['humss']:.4f}",
                f"{result['applied']:.4f}",
                f"{result['other']:.4f}",
                result['best']['name'],
                f"{result['best']['score']:.4f}",
            ]
            
            # ìµœí•˜ìœ„ 10ê°œ ê³¼ëª© ì¶”ê°€
            for worst in result.get('worst_10', []):
                row.extend([worst['name'], f"{worst['score']:.4f}"])
            
            # 10ê°œ ë¯¸ë§Œì´ë©´ ë¹ˆ ì¹¸ ì±„ìš°ê¸°
            remaining = 10 - len(result.get('worst_10', []))
            for _ in range(remaining):
                row.extend(['', ''])
            
            row.append(result['timestamp'])
            writer.writerow(row)

def evaluate_model(model_name, model_args, label):
    """ëª¨ë¸ í‰ê°€ ë° ì €ì¥"""
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    gc.collect()
    
    print(f"\n{'='*60}")
    print(f"Evaluating: {label}")
    print(f"{'='*60}")
    
    
    # CUDA ë©”ëª¨ë¦¬ ìµœì í™”
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    
    # í‰ê°€ ì‹¤í–‰
    results = simple_evaluate(
        model="hf",
        model_args=model_args,
        tasks=["kmmlu"],
        batch_size=8,  # OOM ë°©ì§€
        device="cuda:0"
    )
    
    # ì „ì²´ í‰ê·  ì •í™•ë„
    overall = results['results']['kmmlu']['acc,none']
    
    # â­ 45ê°œ ê°œë³„ ê³¼ëª© ì ìˆ˜ ìˆ˜ì§‘
    all_subjects = {}
    for task, metrics in results['results'].items():
        if task.startswith('kmmlu_') and 'acc,none' in metrics:
            # ëŒ€ë¶„ë¥˜ ì œì™¸, ê°œë³„ ê³¼ëª©ë§Œ
            if task not in ['kmmlu_stem', 'kmmlu_humss', 'kmmlu_applied_science', 'kmmlu_other', 'kmmlu']:
                subject_name = task.replace('kmmlu_', '').replace('_', ' ').title()
                all_subjects[subject_name] = metrics['acc,none']
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
    sorted_subjects = sorted(all_subjects.items(), key=lambda x: x[1], reverse=True)
    
    # â­ í•µì‹¬: ìµœí•˜ìœ„ 10ê°œ ê³¼ëª© (ì ìˆ˜)
    worst_10_subjects = [
        {"name": name, "score": score}
        for name, score in sorted_subjects[-10:][::-1]  # ë§ˆì§€ë§‰ 10ê°œë¥¼ ì—­ìˆœìœ¼ë¡œ (ë‚®ì€ ìˆœ)
    ]
    
    # ê²°ê³¼ ì •ë¦¬
    result_data = {
        "model": label,
        "model_path": model_name,
        "overall": overall,
        "stem": results['results']['kmmlu_stem']['acc,none'],
        "humss": results['results']['kmmlu_humss']['acc,none'],
        "applied": results['results']['kmmlu_applied_science']['acc,none'],
        "other": results['results']['kmmlu_other']['acc,none'],
        "subjects": dict(sorted_subjects),  # ì „ì²´ ê³¼ëª©
        "best": {
            "name": sorted_subjects[0][0],
            "score": sorted_subjects[0][1]
        },
        "worst_10": worst_10_subjects,  # â­ ìµœí•˜ìœ„ 10ê°œ
        "worst": {  # í•˜ìœ„ í˜¸í™˜ìš©
            "name": sorted_subjects[-1][0],
            "score": sorted_subjects[-1][1]
        },
        "timestamp": datetime.now().isoformat()
    }
    

    
    # â­ ì½˜ì†”ì— ìµœí•˜ìœ„ 10ê°œ ì¶œë ¥
    print(f"\nâœ… Evaluation Complete!")
    print(f"Overall: {overall:.2%}")
    print(f"Best:    {result_data['best']['name']} ({result_data['best']['score']:.2%})")
    
    print(f"\nğŸ“‰ Worst 10 Subjects (ë‚®ì€ ì ìˆ˜):")
    for i, subject in enumerate(worst_10_subjects, 1):
        print(f"  {i:2d}. {subject['name']:35s} {subject['score']:.2%}")
    
    print(f"\nTotal Subjects: {len(all_subjects)}")
    
    # ì €ì¥
    all_results = load_results()
    
    # ê¸°ì¡´ ëª¨ë¸ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì¶”ê°€
    existing_idx = None
    for i, r in enumerate(all_results):
        if r.get('Model') == label or r.get('model_path') == model_name:
            existing_idx = i
            break
    
    if existing_idx is not None:
        all_results[existing_idx] = result_data
    else:
        all_results.append(result_data)
    
    save_results(all_results)
    print(f"Saved to {CSV_FILE}\n")
    
    return result_data

if __name__ == "__main__":
    evaluate_model(
        model_name="naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B",
        model_args="pretrained=naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B,dtype=float16",
        label="naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B (16bit)"
    )